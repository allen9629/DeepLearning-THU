{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\allen\\Homework2-MLP\\solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.0457\t Accuracy 0.1900\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0135\t Accuracy 0.2529\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0121\t Accuracy 0.3510\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0103\t Accuracy 0.4276\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0087\t Accuracy 0.4939\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0075\t Accuracy 0.5431\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0066\t Accuracy 0.5805\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0058\t Accuracy 0.6105\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0052\t Accuracy 0.6358\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0047\t Accuracy 0.6555\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0043\t Accuracy 0.6724\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0040\t Average training accuracy 0.6871\n",
      "Epoch [0]\t Average validation loss -0.0013\t Average validation accuracy 0.8698\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss -0.0002\t Accuracy 0.8800\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0004\t Accuracy 0.8453\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0003\t Accuracy 0.8478\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0004\t Accuracy 0.8417\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0003\t Accuracy 0.8437\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0003\t Accuracy 0.8459\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0003\t Accuracy 0.8471\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0003\t Accuracy 0.8466\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0003\t Accuracy 0.8489\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0002\t Accuracy 0.8496\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0002\t Accuracy 0.8508\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0002\t Average training accuracy 0.8524\n",
      "Epoch [1]\t Average validation loss -0.0006\t Average validation accuracy 0.8988\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0002\t Accuracy 0.8773\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0002\t Accuracy 0.8734\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0002\t Accuracy 0.8673\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0002\t Accuracy 0.8687\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0002\t Accuracy 0.8698\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0002\t Accuracy 0.8704\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0002\t Accuracy 0.8696\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0002\t Accuracy 0.8713\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0002\t Accuracy 0.8720\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0002\t Accuracy 0.8724\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0002\t Average training accuracy 0.8735\n",
      "Epoch [2]\t Average validation loss -0.0002\t Average validation accuracy 0.9136\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0006\t Accuracy 0.9100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0003\t Accuracy 0.8965\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0003\t Accuracy 0.8899\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0002\t Accuracy 0.8837\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0002\t Accuracy 0.8857\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0002\t Accuracy 0.8867\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0002\t Accuracy 0.8862\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0002\t Accuracy 0.8851\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0002\t Accuracy 0.8859\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0002\t Accuracy 0.8863\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0002\t Accuracy 0.8864\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0002\t Average training accuracy 0.8873\n",
      "Epoch [3]\t Average validation loss -0.0001\t Average validation accuracy 0.9208\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0006\t Accuracy 0.9300\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0002\t Accuracy 0.9065\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0002\t Accuracy 0.8990\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0002\t Accuracy 0.8935\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0002\t Accuracy 0.8952\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0002\t Accuracy 0.8960\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0002\t Accuracy 0.8957\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0002\t Accuracy 0.8950\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0002\t Accuracy 0.8954\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0002\t Accuracy 0.8958\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0002\t Accuracy 0.8955\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0002\t Average training accuracy 0.8962\n",
      "Epoch [4]\t Average validation loss -0.0001\t Average validation accuracy 0.9254\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0006\t Accuracy 0.9300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0002\t Accuracy 0.9143\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0002\t Accuracy 0.9060\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0002\t Accuracy 0.9014\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0002\t Accuracy 0.9035\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0002\t Accuracy 0.9037\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0002\t Accuracy 0.9032\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0002\t Accuracy 0.9026\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0002\t Accuracy 0.9027\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0002\t Accuracy 0.9031\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0002\t Accuracy 0.9027\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0002\t Average training accuracy 0.9032\n",
      "Epoch [5]\t Average validation loss -0.0001\t Average validation accuracy 0.9306\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0005\t Accuracy 0.9400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0002\t Accuracy 0.9188\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0002\t Accuracy 0.9104\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0002\t Accuracy 0.9061\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9082\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9084\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9079\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9072\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9073\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9077\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9070\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0001\t Average training accuracy 0.9074\n",
      "Epoch [6]\t Average validation loss -0.0002\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0003\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9227\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9139\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9097\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9116\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9120\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9114\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9108\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9108\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9112\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9105\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0001\t Average training accuracy 0.9107\n",
      "Epoch [7]\t Average validation loss -0.0002\t Average validation accuracy 0.9388\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9251\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9172\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9130\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9147\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9153\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9150\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9144\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9144\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9148\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9141\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0001\t Average training accuracy 0.9142\n",
      "Epoch [8]\t Average validation loss -0.0002\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0001\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9263\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9198\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9158\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9176\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9180\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9178\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9171\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9169\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9173\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9167\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0001\t Average training accuracy 0.9167\n",
      "Epoch [9]\t Average validation loss -0.0002\t Average validation accuracy 0.9414\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0001\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9275\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9217\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9177\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9196\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9201\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9197\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9190\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9189\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9192\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9184\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0001\t Average training accuracy 0.9184\n",
      "Epoch [10]\t Average validation loss -0.0002\t Average validation accuracy 0.9430\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0000\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9282\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9231\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9193\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9209\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9214\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9211\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9204\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9203\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9207\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9199\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0001\t Average training accuracy 0.9198\n",
      "Epoch [11]\t Average validation loss -0.0002\t Average validation accuracy 0.9432\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss -0.0000\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9296\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9247\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9204\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9221\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9227\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9224\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9217\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9217\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9221\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9212\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0001\t Average training accuracy 0.9212\n",
      "Epoch [12]\t Average validation loss -0.0002\t Average validation accuracy 0.9432\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss -0.0000\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9300\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9255\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9215\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9232\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9237\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9235\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9229\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9229\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9233\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9225\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0001\t Average training accuracy 0.9223\n",
      "Epoch [13]\t Average validation loss -0.0002\t Average validation accuracy 0.9436\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss -0.0001\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9310\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9268\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9226\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9242\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9246\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9245\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9238\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9238\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9242\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9233\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0001\t Average training accuracy 0.9233\n",
      "Epoch [14]\t Average validation loss -0.0002\t Average validation accuracy 0.9444\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss -0.0001\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9318\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9276\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9234\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9248\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9253\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9251\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9245\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9244\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9248\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9240\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0001\t Average training accuracy 0.9239\n",
      "Epoch [15]\t Average validation loss -0.0002\t Average validation accuracy 0.9444\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss -0.0001\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9322\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9283\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9244\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9258\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9263\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9261\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9255\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9254\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9258\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9250\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0001\t Average training accuracy 0.9249\n",
      "Epoch [16]\t Average validation loss -0.0002\t Average validation accuracy 0.9454\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss -0.0001\t Accuracy 0.9500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9327\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9291\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9251\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9267\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9271\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9268\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9261\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9260\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9264\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9255\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0001\t Average training accuracy 0.9254\n",
      "Epoch [17]\t Average validation loss -0.0002\t Average validation accuracy 0.9468\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss -0.0001\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9337\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9300\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9260\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9274\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9276\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9274\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9266\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9265\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9269\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9260\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0001\t Average training accuracy 0.9260\n",
      "Epoch [18]\t Average validation loss -0.0002\t Average validation accuracy 0.9462\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss -0.0001\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.9347\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.9307\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.9267\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.9280\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.9282\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.9279\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.9271\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.9271\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.9275\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.9266\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0001\t Average training accuracy 0.9265\n",
      "Epoch [19]\t Average validation loss -0.0001\t Average validation accuracy 0.9468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9324.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.0831\t Accuracy 0.0800\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0023\t Accuracy 0.2980\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0019\t Accuracy 0.4176\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0018\t Accuracy 0.4807\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0016\t Accuracy 0.5281\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0016\t Accuracy 0.5656\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0015\t Accuracy 0.5929\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0015\t Accuracy 0.6147\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0014\t Accuracy 0.6340\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0014\t Accuracy 0.6488\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0013\t Accuracy 0.6623\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0013\t Average training accuracy 0.6743\n",
      "Epoch [0]\t Average validation loss 0.0001\t Average validation accuracy 0.8432\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0003\t Accuracy 0.8600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0008\t Accuracy 0.8125\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0009\t Accuracy 0.8139\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0009\t Accuracy 0.8080\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0009\t Accuracy 0.8081\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0009\t Accuracy 0.8096\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0009\t Accuracy 0.8100\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0009\t Accuracy 0.8101\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0009\t Accuracy 0.8117\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0009\t Accuracy 0.8124\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0009\t Accuracy 0.8127\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0009\t Average training accuracy 0.8139\n",
      "Epoch [1]\t Average validation loss 0.0008\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0015\t Accuracy 0.9100\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0008\t Accuracy 0.8369\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0008\t Accuracy 0.8353\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0008\t Accuracy 0.8282\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0008\t Accuracy 0.8274\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0008\t Accuracy 0.8284\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0008\t Accuracy 0.8287\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0008\t Accuracy 0.8279\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0008\t Accuracy 0.8295\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0008\t Accuracy 0.8299\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0008\t Accuracy 0.8299\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0008\t Average training accuracy 0.8308\n",
      "Epoch [2]\t Average validation loss 0.0010\t Average validation accuracy 0.8794\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0017\t Accuracy 0.9000\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0007\t Accuracy 0.8455\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0008\t Accuracy 0.8429\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0008\t Accuracy 0.8365\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0008\t Accuracy 0.8370\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0008\t Accuracy 0.8380\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0008\t Accuracy 0.8380\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0008\t Accuracy 0.8368\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0008\t Accuracy 0.8381\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0008\t Accuracy 0.8383\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0008\t Accuracy 0.8380\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0008\t Average training accuracy 0.8388\n",
      "Epoch [3]\t Average validation loss 0.0011\t Average validation accuracy 0.8830\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0017\t Accuracy 0.9100\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0007\t Accuracy 0.8518\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0008\t Accuracy 0.8486\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0008\t Accuracy 0.8427\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0008\t Accuracy 0.8430\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0008\t Accuracy 0.8441\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0008\t Accuracy 0.8443\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0008\t Accuracy 0.8430\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0008\t Accuracy 0.8442\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0008\t Accuracy 0.8445\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0008\t Accuracy 0.8442\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0008\t Average training accuracy 0.8450\n",
      "Epoch [4]\t Average validation loss 0.0011\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0017\t Accuracy 0.9100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0007\t Accuracy 0.8576\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8550\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8493\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8490\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0008\t Accuracy 0.8499\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0008\t Accuracy 0.8487\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0008\t Accuracy 0.8500\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8504\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0008\t Accuracy 0.8500\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0007\t Average training accuracy 0.8507\n",
      "Epoch [5]\t Average validation loss 0.0011\t Average validation accuracy 0.8912\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0016\t Accuracy 0.9200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0007\t Accuracy 0.8641\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8602\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8545\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8550\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8561\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8558\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8546\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8555\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8559\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8553\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0007\t Average training accuracy 0.8560\n",
      "Epoch [6]\t Average validation loss 0.0011\t Average validation accuracy 0.8932\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0016\t Accuracy 0.9300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0007\t Accuracy 0.8659\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8630\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8582\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8591\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8607\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8605\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8594\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8604\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8607\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8600\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0007\t Average training accuracy 0.8606\n",
      "Epoch [7]\t Average validation loss 0.0011\t Average validation accuracy 0.8974\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0015\t Accuracy 0.9300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0007\t Accuracy 0.8698\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8665\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8627\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8635\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8649\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8647\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8636\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8649\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8652\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8646\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0007\t Average training accuracy 0.8651\n",
      "Epoch [8]\t Average validation loss 0.0011\t Average validation accuracy 0.9000\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0015\t Accuracy 0.9300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0007\t Accuracy 0.8737\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8714\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8677\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8686\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8696\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8692\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8682\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8694\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8695\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8691\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0007\t Average training accuracy 0.8696\n",
      "Epoch [9]\t Average validation loss 0.0011\t Average validation accuracy 0.9032\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0015\t Accuracy 0.9300\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0007\t Accuracy 0.8806\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8774\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8730\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8738\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8747\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8746\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8737\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8745\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8745\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8741\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0007\t Average training accuracy 0.8747\n",
      "Epoch [10]\t Average validation loss 0.0011\t Average validation accuracy 0.9068\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0014\t Accuracy 0.9300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.8857\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8822\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8779\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8783\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8792\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8788\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8779\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8785\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8785\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8781\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0007\t Average training accuracy 0.8788\n",
      "Epoch [11]\t Average validation loss 0.0011\t Average validation accuracy 0.9090\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0014\t Accuracy 0.9400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.8908\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8869\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8830\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8836\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8843\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8837\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8826\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8830\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8830\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8826\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0007\t Average training accuracy 0.8831\n",
      "Epoch [12]\t Average validation loss 0.0011\t Average validation accuracy 0.9124\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0014\t Accuracy 0.9400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.8961\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8909\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8866\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8869\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8874\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8869\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8858\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8863\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8861\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8856\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0007\t Average training accuracy 0.8858\n",
      "Epoch [13]\t Average validation loss 0.0010\t Average validation accuracy 0.9150\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0014\t Accuracy 0.9400\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.8992\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8938\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8898\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0007\t Accuracy 0.8897\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8904\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8900\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8890\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8893\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8892\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8884\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0007\t Average training accuracy 0.8887\n",
      "Epoch [14]\t Average validation loss 0.0010\t Average validation accuracy 0.9174\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0013\t Accuracy 0.9400\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.9016\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8964\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8926\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0006\t Accuracy 0.8928\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0007\t Accuracy 0.8937\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8931\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8919\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8923\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8922\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8914\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0007\t Average training accuracy 0.8916\n",
      "Epoch [15]\t Average validation loss 0.0009\t Average validation accuracy 0.9192\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0013\t Accuracy 0.9400\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.9043\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0007\t Accuracy 0.8994\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8954\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0006\t Accuracy 0.8956\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0006\t Accuracy 0.8959\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8956\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8945\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8949\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8949\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8940\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0007\t Average training accuracy 0.8941\n",
      "Epoch [16]\t Average validation loss 0.0009\t Average validation accuracy 0.9220\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0013\t Accuracy 0.9400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.9067\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0006\t Accuracy 0.9013\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8977\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0006\t Accuracy 0.8979\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0006\t Accuracy 0.8980\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.8980\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8968\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8973\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8973\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8964\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0006\t Average training accuracy 0.8966\n",
      "Epoch [17]\t Average validation loss 0.0009\t Average validation accuracy 0.9244\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0012\t Accuracy 0.9400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.9086\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0006\t Accuracy 0.9038\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0006\t Accuracy 0.9001\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0006\t Accuracy 0.9005\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0006\t Accuracy 0.9002\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0007\t Accuracy 0.9001\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0007\t Accuracy 0.8987\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0007\t Accuracy 0.8992\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.8992\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0007\t Accuracy 0.8984\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0006\t Average training accuracy 0.8985\n",
      "Epoch [18]\t Average validation loss 0.0008\t Average validation accuracy 0.9256\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0012\t Accuracy 0.9400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.9108\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0006\t Accuracy 0.9058\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0006\t Accuracy 0.9022\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0006\t Accuracy 0.9022\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0006\t Accuracy 0.9024\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0006\t Accuracy 0.9023\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0006\t Accuracy 0.9008\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0006\t Accuracy 0.9011\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0006\t Accuracy 0.9011\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0006\t Accuracy 0.9004\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0006\t Average training accuracy 0.9004\n",
      "Epoch [19]\t Average validation loss 0.0008\t Average validation accuracy 0.9276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9104.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEICAYAAABI7RO5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAdaUlEQVR4nO3df5xVdb3v8dcbhh8K/mbwIKiDgT1EMcIR8ZaK+CP1pGjpQapzNTXLwkpvHbXu9SodT5od7fTIx+ly/Zl1UNP0INkx0w51zZAB0QJEJ+PoBCW/NEdFGPjcP9aaYdjsvdlrmLVnD7yfj8d+zPrx/X7XZ282+73XWnuvrYjAzMysUn16ugAzM+tdHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmeQaHJJOk7RMUrOkq4usP17SQkltks4tWHeBpJfT2wV51mlmZpVTXt/jkNQXeAk4BWgB5gPTImJJpzYNwJ7AV4DZEfFgunxfoAloBAJYABwVEetKbW/IkCHR0NCQx10xM9tpLViwYHVE1GfpU5dXMcAEoDkiXgGQdB8wBegIjohYnq7bXND3I8ATEbE2Xf8EcBowq9TGGhoaaGpq6s76zcx2epL+K2ufPA9VDQde6zTfki7Lu6+ZmeUoz+BQkWWVHherqK+kSyU1SWpatWpVpuLMzKxr8gyOFuDATvMjgBXd2TciZkZEY0Q01tdnOkRnZmZdlOc5jvnAaEkjgT8B5wOfqLDv48A/SdonnT8VuKb7SzSz3mzjxo20tLSwfv36ni6l5g0cOJARI0bQr1+/HR4rt+CIiDZJ00lCoC9wZ0QsljQDaIqI2ZKOBh4G9gHOlHR9RBweEWslfYMkfABmtJ8oNzNr19LSwh577EFDQwNSsSPcBhARrFmzhpaWFkaOHLnD4+W5x0FEPAY8VrDs2k7T80kOQxXreydwZ571mVnvtn79eodGBSSx33770V3ngv3NcTPr1RwalenOx8nBYWZmmTg4zMx2wA033MDhhx/OkUceybhx45g3bx6XXHIJS5Ys2X7nHXDGGWfwxhtvbLP8uuuu49vf/nau2871HIeZWa1o/McnWN26YZvlQwb3p+l/ntKlMZ955hnmzJnDwoULGTBgAKtXr2bDhg3cfvvtO1rudj322GPbb5QT73GY2S6hWGiUW16JlStXMmTIEAYMGADAkCFDOOCAA5g0aVLHJZDuuOMODj30UCZNmsRnPvMZpk+fDsCFF17IZZddxoknnsghhxzC3LlzueiiizjssMO48MILO7Yxa9Ysxo4dyxFHHMFVV13VsbyhoYHVq1cDyV7P+9//fk4++WSWLVvW5ftTKe9xmNlO4fpHF7NkxV+71Hfq/3mm6PIxB+zJ/z7z8JL9Tj31VGbMmMGhhx7KySefzNSpUznhhBM61q9YsYJvfOMbLFy4kD322IPJkyfzgQ98oGP9unXreOqpp5g9ezZnnnkmTz/9NLfffjtHH300ixYtYujQoVx11VUsWLCAffbZh1NPPZVHHnmEs88+u2OMBQsWcN999/Hcc8/R1tbG+PHjOeqoo7r0OFTKexxmZl00ePBgFixYwMyZM6mvr2fq1KncfffdHeufffZZTjjhBPbdd1/69evHeeedt1X/M888E0mMHTuW/fffn7Fjx9KnTx8OP/xwli9fzvz585k0aRL19fXU1dXxyU9+kl/96ldbjfHrX/+ac845h913350999yTs846K/f77T0OM9splNszAGi4+qcl193/2WO7vN2+ffsyadIkJk2axNixY7nnnns61m3vZyvaD3H16dOnY7p9vq2tjbq6yl6iq/2RZO9xmJl10bJly3j55Zc75hctWsTBBx/cMT9hwgTmzp3LunXraGtr46GHHso0/jHHHMPcuXNZvXo1mzZtYtasWVsdCgM4/vjjefjhh3n33Xd56623ePTRR3fsTlXAexxmtksYMrh/yU9VdVVrayuXX345b7zxBnV1dYwaNYqZM2dy7rnJD5oOHz6cr33taxxzzDEccMABjBkzhr322qvi8YcNG8Y3v/lNTjzxRCKCM844gylTpmzVZvz48UydOpVx48Zx8MEHc9xxx3X5/lQqt18ArLbGxsbwDzmZ7VqWLl3KYYcd1tNllNXa2srgwYNpa2vjnHPO4aKLLuKcc87pkVqKPV6SFkREY5ZxfKjKzCxH1113HePGjeOII45g5MiRW30iqrfyoSozsxzl/S3unuA9DjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM6uCzhc+7O38qSoz2zXcPBrefn3b5YOGwldf3nZ5F0QEEUGfPjv3e/Kd+96ZmbUrFhrllldo+fLlHHbYYXz+859n/Pjx3HvvvRx77LGMHz+e8847j9bW1m36DB48uGP6wQcf3Ooy6r2B9zjMbOfws6vhz7/rWt+7/rb48r8ZC6ffuN3uy5Yt46677mLGjBl87GMf4xe/+AWDBg3ipptu4pZbbuHaa6/tWl01ysFhZraDDj74YCZOnMicOXNYsmQJH/rQhwDYsGEDxx7b9Svv1ioHh5ntHLa3Z3BdmYsLfrr0JdcrMWjQICA5x3HKKacwa9assu07XwZ9/fr1O7TtnuBzHGZm3WTixIk8/fTTNDc3A/DOO+/w0ksvbdNu//33Z+nSpWzevJmHH3642mXuMAeHme0aBg3NtrwL6uvrufvuu5k2bRpHHnkkEydO5MUXX9ym3Y033shHP/pRJk+ezLBhw7pt+9Xiy6qbWa/VGy6rXkt8WXUzM+sRDg4zM8vEwWFmvdrOcrg9b935ODk4zKzXGjhwIGvWrHF4bEdEsGbNGgYOHNgt4/l7HGbWa40YMYKWlhZWrVrV06XUvIEDBzJixIhuGcvBYWa9Vr9+/Rg5cmRPl7HL8aEqMzPLxMFhZmaZ5Bockk6TtExSs6Sri6wfIOn+dP08SQ3p8n6S7pH0O0lLJV2TZ51mZla53IJDUl/gNuB0YAwwTdKYgmYXA+siYhRwK3BTuvw8YEBEjAWOAj7bHipmZtaz8tzjmAA0R8QrEbEBuA+YUtBmCnBPOv0gcJKSy0YGMEhSHbAbsAH4a461mplZhfIMjuHAa53mW9JlRdtERBvwJrAfSYi8DawEXgW+HRFrc6zVzMwqlGdwqMiywm/plGozAdgEHACMBP6HpEO22YB0qaQmSU3+HLeZWXXkGRwtwIGd5kcAK0q1SQ9L7QWsBT4B/EdEbIyI14GngW2u3hgRMyOiMSIa6+vrc7gLZmZWKM/gmA+MljRSUn/gfGB2QZvZwAXp9LnAU5FcO+BVYLISg4CJwLYXtTczs6rLLTjScxbTgceBpcADEbFY0gxJZ6XN7gD2k9QMXAm0f2T3NmAw8HuSALorIl7Iq1YzM6ucf8jJzGwX5h9yMjOz3Dk4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsk1yDQ9JpkpZJapZ0dZH1AyTdn66fJ6mh07ojJT0jabGk30kamGetZmZWmdyCQ1Jf4DbgdGAMME3SmIJmFwPrImIUcCtwU9q3Dvgh8LmIOByYBGzMq1YzM6tcnnscE4DmiHglIjYA9wFTCtpMAe5Jpx8ETpIk4FTghYh4HiAi1kTEphxrNTOzCuUZHMOB1zrNt6TLiraJiDbgTWA/4FAgJD0uaaGkf8ixTjMzy6Aux7FVZFlU2KYO+DBwNPAO8KSkBRHx5FadpUuBSwEOOuigHS7YzMy2L889jhbgwE7zI4AVpdqk5zX2Atamy+dGxOqIeAd4DBhfuIGImBkRjRHRWF9fn8NdMDOzQnkGx3xgtKSRkvoD5wOzC9rMBi5Ip88FnoqIAB4HjpS0exooJwBLcqzVzMwqlNuhqohokzSdJAT6AndGxGJJM4CmiJgN3AHcK6mZZE/j/LTvOkm3kIRPAI9FxE/zqtXMzCqn5A1+79fY2BhNTU09XYaZWa+Snj9uzNLH3xw3M7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmVQUHJLeJ2lAOj1J0hcl7Z1vaWZmVosq3eN4CNgkaRTJl/ZGAv+WW1VmZlazKg2OzenVa88BvhMRVwDD8ivLzMxqVaXBsVHSNJLrSs1Jl/XLpyQzM6tllQbHp4FjgRsi4o+SRpL8Qp+Zme1iKrrIYUQsAb4IIGkfYI+IuDHPwszMrDZV+qmq/5S0p6R9geeBu9Kr15qZ2S6m0kNVe0XEX4GPAXdFxFHAyfmVZWZmtarS4KiTNAz4O7acHDczs11QpcExg+QHmf4QEfMlHQK8nF9ZZmZWqyo9Of5j4Med5l8BPp5XUWZmVrsqPTk+QtLDkl6X9BdJD0kakXdxZmZWeyo9VHUXMBs4ABgOPJouMzOzXUylwVEfEXdFRFt6uxuoz7EuMzOrUZUGx2pJn5LUN719CliTZ2FmZlabKg2Oi0g+ivtnYCVwLsllSMzMbBdTUXBExKsRcVZE1EfE0Ig4m+TLgGZmtovZkV8AvLLbqjAzs15jR4JD3VaFmZn1GjsSHNFtVZiZWa9R9pvjkt6ieEAI2C2XiszMrKaVDY6I2KNahZiZWe+wI4eqzMxsF+TgMDOzTBwcZmaWiYPDzMwycXCYmVkmDg4zM8sk1+CQdJqkZZKaJV1dZP0ASfen6+dJaihYf5CkVklfybNOMzOrXG7BIakvcBtwOjAGmCZpTEGzi4F1ETEKuBW4qWD9rcDP8qrRzMyyy3OPYwLQHBGvRMQG4D5gSkGbKcA96fSDwEmSBCDpbOAVYHGONZqZWUZ5Bsdw4LVO8y3psqJtIqINeBPYT9Ig4Crg+hzrMzOzLsgzOIpdPbfwulel2lwP3BoRrWU3IF0qqUlS06pVq7pYppmZZVH2WlU7qAU4sNP8CGBFiTYtkuqAvYC1wDHAuZK+BewNbJa0PiK+17lzRMwEZgI0Njb6ar1mZlWQZ3DMB0ZLGgn8CTgf+ERBm9nABcAzJD9H+1REBHBcewNJ1wGthaFhZmY9I7fgiIg2SdOBx4G+wJ0RsVjSDKApImYDdwD3Smom2dM4P696zMyseyh5g9/7NTY2RlNTU0+XYWbWq0haEBGNWfr4m+NmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zMwsEweHmZll4uAwM7NMHBxmZpaJg8PMzDLJNTgknSZpmaRmSVcXWT9A0v3p+nmSGtLlp0haIOl36d/JedZpZmaVyy04JPUFbgNOB8YA0ySNKWh2MbAuIkYBtwI3pctXA2dGxFjgAuDevOo0M7Ns8tzjmAA0R8QrEbEBuA+YUtBmCnBPOv0gcJIkRcRzEbEiXb4YGChpQI61mplZhfIMjuHAa53mW9JlRdtERBvwJrBfQZuPA89FxHs51WlmZhnU5Ti2iiyLLG0kHU5y+OrUohuQLgUuBTjooIO6VqWZmWWS5x5HC3Bgp/kRwIpSbSTVAXsBa9P5EcDDwH+PiD8U20BEzIyIxohorK+v7+byzcysmDyDYz4wWtJISf2B84HZBW1mk5z8BjgXeCoiQtLewE+BayLi6RxrNDOzjHILjvScxXTgcWAp8EBELJY0Q9JZabM7gP0kNQNXAu0f2Z0OjAL+l6RF6W1oXrWamVnlFFF42qF3amxsjKampp4uw8ysV5G0ICIas/TxN8fNzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZODjMzCwTB4eZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPLxMFhZmaZ1PV0AdZNbh4Nb7++7fJBQ+GrL+9aY9RCDd0xRi3UYFaEg6M71MJ/8GJ9yy3fmceohRq6Y4xaqAFq4/ldCzXUyhjdXMNRw/ocVVmnLXINDkmnAf8C9AVuj4gbC9YPAH4AHAWsAaZGxPJ03TXAxcAm4IsR8Xiete6QrP85I2DThuTWtgE2vVd+jNeehdhc5hbl61v8CBBbtr3VNNvvD7Bo1pZ+2/SpcJyFPyjRpoLtt1twd+VtS/bX1stUMF+4vtBzP+p633YvPLBt+23GKmPxI536qPx0KS//Im2iLe2L/S333Gxp2tKuq2OsfaWgfZ9txyjXf/2bZfr3qWyMStXCGHnWUCFFJS8aXRlY6gu8BJwCtADzgWkRsaRTm88DR0bE5ySdD5wTEVMljQFmAROAA4BfAIdGxKZS22tsbIympqbshWZJ7wh4dx20vp70aU1vj19Tevx9GraEQ/vfTRuy12lm+enbPwmZzmHTMc+W6XfXlh5jrwOLhGaREFv1YukxDvhgmf7p31d/U7r/+04qHtwd9yV9M/HinI4ujTNbaVqxKcO7lnz3OCYAzRHxCoCk+4ApwJJObaYA16XTDwLfk6R0+X0R8R7wR0nN6XjPdHuV5dL7378Araug9S/w9qokJDZvzDb+gcckT8q+/aFuQPG/ffvDnC+XHuNTD6VPoDK3208q3f+y37DVk6botOB7ZfZYv7gobd75+VXk3fJ3xpYe44rF2/Yr7A9wy2Glx7hyael1lfS/YknBgoI3Tu1vpL5zROkxvvRC+b7t6777wdJjXL5w+3tttx1duv9lzyR9OtqXmJ55QukxLn5iyx5oub8/OKv0GJ/48ZZ2sbn0GD++oPQYZ39/2/aFY825onT/j/xTpz3vIv3bl//yhtJjTPx82n7z1n22GnczzL+99BgNxxXpU1jL5vLBsfuQ7TwW27H+jfL/lp2POOyAPINjOPBap/kW4JhSbSKiTdKbwH7p8t8W9B2eX6klND8Jg4cmex/7HwGD62Hw/jAo/Tt4aHK7qaH0GB+byXttm3j7vU20rm/jrfc20rq+jbc3tPHW+jZa32vj7XfauLRMGZ97Zh/aNgebI9jU6W8y3camzcFPyvQ//b7kXdJWL/lFXvPnUNrH71+ZvPESSEJAHwlpy19J/KDMGJc9+pfS/UmW9xHcXGaMa3+5lj5pwRKIpE/nccvs/3HLvFbo2F7Sf8tYWx6L6WXG+P4LbR2PZXv7zuO0u6TMGHcu3fYDjYVHqj5dpv/dzbuVWbvFhWXW3fVqfcnXos6LLy4zxu1/GZW073wUs+CFKQI+W2aMf31jwlZ9itX0hTL9/+XtU7fabscR2IIBr6R0cHxr07SO9lv6F9wPwdcoHRzX103fqn8yHVvVEgHfYHbJMa7Z/dqta4ht79fNHF+y/5f3+OeO7XV+HLaqI+C2108sOUYl8jxUdR7wkYi4JJ3/e2BCRFzeqc3itE1LOv8Hkj2LGcAzEfHDdPkdwGMR8VDBNi6FjtfcI4DfZ62z3ImhBSs3L+gtY3xgWL+j6tj2SF4bfXl+5caKathZxqiFGrpjjFqoAWrj+V0LNdTKGN1dw/I3NrP6naiZQ1UtwIGd5kcAK0q0aZFUB+wFrK2wLxExE5gJIKkpIhq7rfouqoU6aqGGWqmjFmqolTpqoYZaqaMWaqiVOiRlPjmc5xcA5wOjJY2U1B84H7bZR5sNtB/8PBd4KpJdoNnA+ZIGSBoJjAaezbFWMzOrUG57HOk5i+nA4yQfx70zIhZLmgE0RcRs4A7g3vTk91qScCFt9wDJifQ24AvlPlFlZmbVk+v3OCLiMeCxgmXXdppeD5xXou8NUOZs1rZmdqXGHNRCHbVQA9RGHbVQA9RGHbVQA9RGHbVQA9RGHZlryO3kuJmZ7Zx8kUMzM8tkpwgOSadJWiapWdLVPbD9AyX9UtJSSYslfanaNRTU01fSc5LKfTUjz+3vLelBSS+mj8mxPVTHFem/x+8lzZI0sErbvVPS65J+32nZvpKekPRy+nefHqjh5vTf5AVJD0vau9o1dFr3FUkhaUieNZSrQ9Ll6evGYknfqnYNksZJ+q2kRZKaJE3Is4Z0m0VfqzI/PyOiV99ITrz/ATgE6A88D4ypcg3DgPHp9B4kl1qpag0F9VwJ/Bswp4e2fw9wSTrdH9i7B2oYDvwR2C2dfwC4sErbPh4YD/y+07JvAVen01cDN/VADacCden0TT1RQ7r8QJIPzfwXMKSH/j1OJLmU0YB0fmgP1PBz4PR0+gzgP6vwWBR9rcr6/NwZ9jg6Lm0SERuA9kubVE1ErIyIhen0W8BSeuKb7oCkEcDfQpmvuOa7/T1J/pPcARARGyLijZ6oheTDH7ul3xHanSLfBcpDRPyK5FOCnU0hCVTSv2dXu4aI+HlEtKWzvyX5flRVa0jdCvwD3XHti67XcRlwYySXNSIiduyqf12rIYA90+m9qMLzs8xrVabn584QHMUubdIjL9oAkhqADwLzeqiE75D8p9zcQ9s/BFgF3JUeLrtd0qBqFxERfwK+DbwKrATejIifV7uOTvaPiJVpbSuBoT1YC8BFwM+qvVFJZwF/iojnq73tAocCx0maJ2mupDIXBsvNl4GbJb1G8lwtd7WcblfwWpXp+bkzBEexr8r3yEfFJA0GHgK+HBF/7YHtfxR4PSIquuxATupIdsn/NSI+CLxNsutbVekx2inASJIrLA+S9Klq11GLJH2d5PtRP9pe227e7u7A14Frt9e2CuqAfYCJwFeBB9ILrFbTZcAVEXEgcAXpXno17Ohr1c4QHBVdniRvkvqR/EP8KCLKXXMwTx8CzpK0nOSQ3WRJP6xyDS1AS0S073E9SBIk1XYy8MeIWBURG4GfAP+tB+po9xdJwwDSv7keGilF0gXAR4FPRnpAu4reRxLkz6fP0RHAQkl/U+U6IHme/iQSz5Lsoed+or7ABdBxfdIfkxx2z12J16pMz8+dITgqubRJrtJ3KncASyPilmpuu7OIuCYiRkREA8nj8FREVPVddkT8GXhN0vvTRSex9aX0q+VVYKKk3dN/n5NIjuf2lM6X17kA+PdqF6Dkh9WuAs6KiHeqvf2I+F1EDI2IhvQ52kJyovbP1a4FeASYDCDpUJIPcayucg0rgPbr3k8Gcv8t3zKvVdmen3mfxa/GjeQTCS+RfLrq6z2w/Q+THB57AViU3s7o4cdkEj33qapxQFP6eDwC7NNDdVwPvEhy1eR7ST9BU4XtziI5r7KR5MXxYpKfC3iS5MXhSWDfHqihmeR8YPtz9PvVrqFg/XKq86mqYo9Ff+CH6XNjITC5B2r4MLCA5JOg84CjqvBYFH2tyvr89DfHzcwsk53hUJWZmVWRg8PMzDJxcJiZWSYODjMzy8TBYWZmmTg4zDKQtCm9mmn7rdu+FS+podiVZM1qTa6/AGi2E3o3Isb1dBFmPcl7HGbdQNJySTdJeja9jUqXHyzpyfQ3MJ6UdFC6fP/0NzGeT2/tl0PpK+n/pr+V8HNJu/XYnTIrwcFhls1uBYeqpnZa99eImAB8j+QqxaTTP4iII0kuKvjddPl3gbkR8QGSa3ktTpePBm6LiMOBN4CP53x/zDLzN8fNMpDUGhGDiyxfTnLZilfSi8j9OSL2k7QaGBYRG9PlKyNiiKRVwIhIfw8iHaMBeCIiRqfzVwH9IuIf879nZpXzHodZ94kS06XaFPNep+lN+Dyk1SAHh1n3mdrp7zPp9G9IrlQM8Eng/6XTT5L8HkP7b8S3/xKcWc3zuxmzbHaTtKjT/H9ERPtHcgdImkfyhmxauuyLwJ2Svkryy4ifTpd/CZgp6WKSPYvLSK6ealbzfI7DrBuk5zgaI6Lav+lgVnU+VGVmZpl4j8PMzDLxHoeZmWXi4DAzs0wcHGZmlomDw8zMMnFwmJlZJg4OMzPL5P8Dx6fH6pq9kjAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss -0.0295\t Accuracy 0.0700\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0162\t Accuracy 0.2351\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0182\t Accuracy 0.3265\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0208\t Accuracy 0.3851\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0236\t Accuracy 0.4338\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0260\t Accuracy 0.4739\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0284\t Accuracy 0.5052\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0304\t Accuracy 0.5300\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0322\t Accuracy 0.5535\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0339\t Accuracy 0.5750\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0355\t Accuracy 0.5948\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0370\t Average training accuracy 0.6133\n",
      "Epoch [0]\t Average validation loss 0.0555\t Average validation accuracy 0.8450\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0552\t Accuracy 0.8600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0545\t Accuracy 0.8206\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0548\t Accuracy 0.8257\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0550\t Accuracy 0.8238\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0555\t Accuracy 0.8296\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0561\t Accuracy 0.8331\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0567\t Accuracy 0.8378\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0570\t Accuracy 0.8393\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0574\t Accuracy 0.8417\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0578\t Accuracy 0.8438\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0581\t Accuracy 0.8456\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0586\t Average training accuracy 0.8478\n",
      "Epoch [1]\t Average validation loss 0.0659\t Average validation accuracy 0.9028\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0658\t Accuracy 0.9200\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0640\t Accuracy 0.8806\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8762\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0635\t Accuracy 0.8703\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0636\t Accuracy 0.8719\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0639\t Accuracy 0.8727\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0641\t Accuracy 0.8741\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0641\t Accuracy 0.8731\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0642\t Accuracy 0.8741\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0644\t Accuracy 0.8752\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0645\t Accuracy 0.8756\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0648\t Average training accuracy 0.8767\n",
      "Epoch [2]\t Average validation loss 0.0701\t Average validation accuracy 0.9150\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0701\t Accuracy 0.9200\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0682\t Accuracy 0.8943\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0680\t Accuracy 0.8902\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0675\t Accuracy 0.8845\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0675\t Accuracy 0.8865\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0677\t Accuracy 0.8873\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0678\t Accuracy 0.8878\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0677\t Accuracy 0.8867\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0678\t Accuracy 0.8873\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0678\t Accuracy 0.8883\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0679\t Accuracy 0.8884\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0681\t Average training accuracy 0.8891\n",
      "Epoch [3]\t Average validation loss 0.0728\t Average validation accuracy 0.9208\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0729\t Accuracy 0.9200\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0709\t Accuracy 0.9049\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0706\t Accuracy 0.8993\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0701\t Accuracy 0.8949\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0701\t Accuracy 0.8974\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0702\t Accuracy 0.8975\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0703\t Accuracy 0.8972\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0701\t Accuracy 0.8962\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0702\t Accuracy 0.8963\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0702\t Accuracy 0.8972\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0702\t Accuracy 0.8971\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0703\t Average training accuracy 0.8975\n",
      "Epoch [4]\t Average validation loss 0.0747\t Average validation accuracy 0.9240\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0748\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0728\t Accuracy 0.9133\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0725\t Accuracy 0.9060\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0719\t Accuracy 0.9019\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0719\t Accuracy 0.9037\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0720\t Accuracy 0.9040\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0720\t Accuracy 0.9039\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0718\t Accuracy 0.9029\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0719\t Accuracy 0.9028\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0718\t Accuracy 0.9035\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0719\t Accuracy 0.9032\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0720\t Average training accuracy 0.9034\n",
      "Epoch [5]\t Average validation loss 0.0761\t Average validation accuracy 0.9302\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0762\t Accuracy 0.9400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0741\t Accuracy 0.9171\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0738\t Accuracy 0.9113\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0732\t Accuracy 0.9077\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0732\t Accuracy 0.9091\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0733\t Accuracy 0.9092\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0733\t Accuracy 0.9093\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0731\t Accuracy 0.9080\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0731\t Accuracy 0.9080\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0731\t Accuracy 0.9086\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0731\t Accuracy 0.9081\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0732\t Average training accuracy 0.9083\n",
      "Epoch [6]\t Average validation loss 0.0772\t Average validation accuracy 0.9332\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0773\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0752\t Accuracy 0.9218\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0749\t Accuracy 0.9161\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0742\t Accuracy 0.9123\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0742\t Accuracy 0.9134\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0743\t Accuracy 0.9135\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0743\t Accuracy 0.9135\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0740\t Accuracy 0.9123\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0741\t Accuracy 0.9121\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0740\t Accuracy 0.9126\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0740\t Accuracy 0.9119\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0741\t Average training accuracy 0.9120\n",
      "Epoch [7]\t Average validation loss 0.0779\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0781\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0759\t Accuracy 0.9249\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0756\t Accuracy 0.9198\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0750\t Accuracy 0.9154\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0750\t Accuracy 0.9166\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0751\t Accuracy 0.9167\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0750\t Accuracy 0.9167\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0748\t Accuracy 0.9157\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0748\t Accuracy 0.9153\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0748\t Accuracy 0.9156\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0747\t Accuracy 0.9149\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0748\t Average training accuracy 0.9150\n",
      "Epoch [8]\t Average validation loss 0.0786\t Average validation accuracy 0.9390\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0787\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0766\t Accuracy 0.9261\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0763\t Accuracy 0.9212\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0757\t Accuracy 0.9173\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0756\t Accuracy 0.9186\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0757\t Accuracy 0.9187\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0757\t Accuracy 0.9187\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0754\t Accuracy 0.9176\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0754\t Accuracy 0.9173\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0754\t Accuracy 0.9175\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0753\t Accuracy 0.9168\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0754\t Average training accuracy 0.9169\n",
      "Epoch [9]\t Average validation loss 0.0791\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0793\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0771\t Accuracy 0.9278\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0768\t Accuracy 0.9230\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0762\t Accuracy 0.9191\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0761\t Accuracy 0.9203\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0762\t Accuracy 0.9206\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0762\t Accuracy 0.9205\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0759\t Accuracy 0.9194\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0759\t Accuracy 0.9192\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0759\t Accuracy 0.9194\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0758\t Accuracy 0.9187\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0759\t Average training accuracy 0.9188\n",
      "Epoch [10]\t Average validation loss 0.0795\t Average validation accuracy 0.9414\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0797\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0775\t Accuracy 0.9284\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0772\t Accuracy 0.9245\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0766\t Accuracy 0.9206\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0765\t Accuracy 0.9218\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0766\t Accuracy 0.9221\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0766\t Accuracy 0.9219\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0763\t Accuracy 0.9209\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0763\t Accuracy 0.9205\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0763\t Accuracy 0.9208\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0762\t Accuracy 0.9200\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0763\t Average training accuracy 0.9201\n",
      "Epoch [11]\t Average validation loss 0.0798\t Average validation accuracy 0.9424\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0801\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0778\t Accuracy 0.9286\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0775\t Accuracy 0.9253\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0770\t Accuracy 0.9216\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0769\t Accuracy 0.9229\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0770\t Accuracy 0.9233\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0769\t Accuracy 0.9232\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0767\t Accuracy 0.9222\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0767\t Accuracy 0.9220\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0766\t Accuracy 0.9221\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0766\t Accuracy 0.9213\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0766\t Average training accuracy 0.9213\n",
      "Epoch [12]\t Average validation loss 0.0801\t Average validation accuracy 0.9430\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0804\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0782\t Accuracy 0.9302\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0778\t Accuracy 0.9267\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0773\t Accuracy 0.9225\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0772\t Accuracy 0.9238\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0773\t Accuracy 0.9241\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0772\t Accuracy 0.9242\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0770\t Accuracy 0.9232\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0770\t Accuracy 0.9229\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0769\t Accuracy 0.9231\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0769\t Accuracy 0.9222\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0769\t Average training accuracy 0.9224\n",
      "Epoch [13]\t Average validation loss 0.0804\t Average validation accuracy 0.9434\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0807\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0784\t Accuracy 0.9310\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0781\t Accuracy 0.9277\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0776\t Accuracy 0.9238\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0775\t Accuracy 0.9249\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0775\t Accuracy 0.9253\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0775\t Accuracy 0.9252\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0773\t Accuracy 0.9243\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0773\t Accuracy 0.9241\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0772\t Accuracy 0.9241\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0771\t Accuracy 0.9233\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0772\t Average training accuracy 0.9234\n",
      "Epoch [14]\t Average validation loss 0.0806\t Average validation accuracy 0.9446\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0809\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0787\t Accuracy 0.9312\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0784\t Accuracy 0.9283\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0778\t Accuracy 0.9243\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0778\t Accuracy 0.9255\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0778\t Accuracy 0.9258\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0778\t Accuracy 0.9258\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0775\t Accuracy 0.9250\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0775\t Accuracy 0.9248\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0775\t Accuracy 0.9248\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0774\t Accuracy 0.9240\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0774\t Average training accuracy 0.9241\n",
      "Epoch [15]\t Average validation loss 0.0808\t Average validation accuracy 0.9452\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0812\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0789\t Accuracy 0.9316\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0786\t Accuracy 0.9292\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0780\t Accuracy 0.9251\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0780\t Accuracy 0.9262\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0780\t Accuracy 0.9265\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0780\t Accuracy 0.9266\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0777\t Accuracy 0.9258\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0777\t Accuracy 0.9256\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0777\t Accuracy 0.9256\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0776\t Accuracy 0.9247\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0777\t Average training accuracy 0.9248\n",
      "Epoch [16]\t Average validation loss 0.0810\t Average validation accuracy 0.9462\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0814\t Accuracy 0.9500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0791\t Accuracy 0.9316\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0788\t Accuracy 0.9293\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0783\t Accuracy 0.9256\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0782\t Accuracy 0.9266\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0782\t Accuracy 0.9270\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0782\t Accuracy 0.9271\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0780\t Accuracy 0.9263\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0780\t Accuracy 0.9261\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0779\t Accuracy 0.9261\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0778\t Accuracy 0.9252\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0779\t Average training accuracy 0.9253\n",
      "Epoch [17]\t Average validation loss 0.0812\t Average validation accuracy 0.9472\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0816\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0793\t Accuracy 0.9318\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0790\t Accuracy 0.9295\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0785\t Accuracy 0.9259\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0784\t Accuracy 0.9269\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0785\t Accuracy 0.9273\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0784\t Accuracy 0.9274\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0782\t Accuracy 0.9267\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0782\t Accuracy 0.9265\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0781\t Accuracy 0.9266\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0780\t Accuracy 0.9257\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0781\t Average training accuracy 0.9258\n",
      "Epoch [18]\t Average validation loss 0.0814\t Average validation accuracy 0.9474\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0818\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0795\t Accuracy 0.9316\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0792\t Accuracy 0.9294\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0787\t Accuracy 0.9262\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0786\t Accuracy 0.9272\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0786\t Accuracy 0.9278\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0786\t Accuracy 0.9280\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0784\t Accuracy 0.9272\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0783\t Accuracy 0.9270\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0783\t Accuracy 0.9270\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0782\t Accuracy 0.9260\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0783\t Average training accuracy 0.9262\n",
      "Epoch [19]\t Average validation loss 0.0816\t Average validation accuracy 0.9476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9329.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.0150\t Accuracy 0.2000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0300\t Accuracy 0.2622\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0365\t Accuracy 0.3724\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0400\t Accuracy 0.4477\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0423\t Accuracy 0.5007\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0440\t Accuracy 0.5414\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0454\t Accuracy 0.5723\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0462\t Accuracy 0.5962\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0471\t Accuracy 0.6179\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0478\t Accuracy 0.6359\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0483\t Accuracy 0.6503\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0489\t Average training accuracy 0.6633\n",
      "Epoch [0]\t Average validation loss 0.0575\t Average validation accuracy 0.8350\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0576\t Accuracy 0.8600\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0556\t Accuracy 0.8075\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0557\t Accuracy 0.8098\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0554\t Accuracy 0.8072\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0553\t Accuracy 0.8109\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0554\t Accuracy 0.8122\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0554\t Accuracy 0.8132\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0553\t Accuracy 0.8133\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0554\t Accuracy 0.8145\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0554\t Accuracy 0.8158\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0554\t Accuracy 0.8159\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0555\t Average training accuracy 0.8170\n",
      "Epoch [1]\t Average validation loss 0.0593\t Average validation accuracy 0.8662\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0594\t Accuracy 0.8500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0574\t Accuracy 0.8357\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0574\t Accuracy 0.8371\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0571\t Accuracy 0.8336\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0570\t Accuracy 0.8372\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0570\t Accuracy 0.8372\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0570\t Accuracy 0.8367\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0568\t Accuracy 0.8355\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0569\t Accuracy 0.8355\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0569\t Accuracy 0.8356\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0569\t Accuracy 0.8347\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0570\t Average training accuracy 0.8350\n",
      "Epoch [2]\t Average validation loss 0.0605\t Average validation accuracy 0.8740\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0606\t Accuracy 0.8600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0586\t Accuracy 0.8469\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0586\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0583\t Accuracy 0.8430\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0582\t Accuracy 0.8459\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0582\t Accuracy 0.8465\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0582\t Accuracy 0.8458\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0580\t Accuracy 0.8448\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0581\t Accuracy 0.8447\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0581\t Accuracy 0.8446\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0581\t Accuracy 0.8437\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0582\t Average training accuracy 0.8437\n",
      "Epoch [3]\t Average validation loss 0.0615\t Average validation accuracy 0.8794\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0618\t Accuracy 0.8600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0597\t Accuracy 0.8533\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0597\t Accuracy 0.8528\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0594\t Accuracy 0.8497\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0593\t Accuracy 0.8524\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0593\t Accuracy 0.8534\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0593\t Accuracy 0.8529\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0591\t Accuracy 0.8519\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0592\t Accuracy 0.8520\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0592\t Accuracy 0.8518\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0592\t Accuracy 0.8508\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0593\t Average training accuracy 0.8509\n",
      "Epoch [4]\t Average validation loss 0.0626\t Average validation accuracy 0.8876\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0630\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0608\t Accuracy 0.8604\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0608\t Accuracy 0.8594\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0604\t Accuracy 0.8560\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0603\t Accuracy 0.8587\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0604\t Accuracy 0.8593\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0604\t Accuracy 0.8588\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0602\t Accuracy 0.8577\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0602\t Accuracy 0.8576\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0602\t Accuracy 0.8573\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0602\t Accuracy 0.8563\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0603\t Average training accuracy 0.8566\n",
      "Epoch [5]\t Average validation loss 0.0637\t Average validation accuracy 0.8924\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0642\t Accuracy 0.8700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0618\t Accuracy 0.8673\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0618\t Accuracy 0.8657\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0615\t Accuracy 0.8623\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0614\t Accuracy 0.8649\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0615\t Accuracy 0.8650\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0615\t Accuracy 0.8647\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0612\t Accuracy 0.8633\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0613\t Accuracy 0.8633\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0613\t Accuracy 0.8628\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0613\t Accuracy 0.8621\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0614\t Average training accuracy 0.8623\n",
      "Epoch [6]\t Average validation loss 0.0647\t Average validation accuracy 0.8958\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0653\t Accuracy 0.8900\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0629\t Accuracy 0.8737\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0629\t Accuracy 0.8714\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0625\t Accuracy 0.8677\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0624\t Accuracy 0.8704\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0625\t Accuracy 0.8705\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0625\t Accuracy 0.8700\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0622\t Accuracy 0.8687\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0623\t Accuracy 0.8686\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0623\t Accuracy 0.8681\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0623\t Accuracy 0.8673\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0624\t Average training accuracy 0.8674\n",
      "Epoch [7]\t Average validation loss 0.0658\t Average validation accuracy 0.9016\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0664\t Accuracy 0.9000\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0639\t Accuracy 0.8782\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.8743\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0635\t Accuracy 0.8707\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0634\t Accuracy 0.8738\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0635\t Accuracy 0.8743\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0635\t Accuracy 0.8739\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0632\t Accuracy 0.8725\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0633\t Accuracy 0.8722\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0633\t Accuracy 0.8715\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0633\t Accuracy 0.8706\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0634\t Average training accuracy 0.8709\n",
      "Epoch [8]\t Average validation loss 0.0668\t Average validation accuracy 0.9062\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0675\t Accuracy 0.9000\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0649\t Accuracy 0.8822\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0649\t Accuracy 0.8787\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0645\t Accuracy 0.8755\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0644\t Accuracy 0.8784\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0645\t Accuracy 0.8792\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0645\t Accuracy 0.8789\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0642\t Accuracy 0.8774\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0643\t Accuracy 0.8769\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0643\t Accuracy 0.8762\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0642\t Accuracy 0.8754\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0643\t Average training accuracy 0.8755\n",
      "Epoch [9]\t Average validation loss 0.0677\t Average validation accuracy 0.9112\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0685\t Accuracy 0.9000\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0659\t Accuracy 0.8880\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0658\t Accuracy 0.8840\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0654\t Accuracy 0.8811\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0653\t Accuracy 0.8833\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0655\t Accuracy 0.8839\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0654\t Accuracy 0.8834\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0651\t Accuracy 0.8819\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0652\t Accuracy 0.8813\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0652\t Accuracy 0.8806\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0651\t Accuracy 0.8797\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0652\t Average training accuracy 0.8798\n",
      "Epoch [10]\t Average validation loss 0.0686\t Average validation accuracy 0.9158\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0694\t Accuracy 0.9000\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0668\t Accuracy 0.8933\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0667\t Accuracy 0.8883\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0663\t Accuracy 0.8852\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0662\t Accuracy 0.8871\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0664\t Accuracy 0.8878\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0663\t Accuracy 0.8870\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0660\t Accuracy 0.8858\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0661\t Accuracy 0.8854\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0660\t Accuracy 0.8844\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0660\t Accuracy 0.8837\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0661\t Average training accuracy 0.8838\n",
      "Epoch [11]\t Average validation loss 0.0695\t Average validation accuracy 0.9174\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0704\t Accuracy 0.9100\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0677\t Accuracy 0.8971\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0676\t Accuracy 0.8921\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0672\t Accuracy 0.8894\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0671\t Accuracy 0.8912\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0672\t Accuracy 0.8917\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0671\t Accuracy 0.8909\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0669\t Accuracy 0.8896\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0669\t Accuracy 0.8893\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0669\t Accuracy 0.8883\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0668\t Accuracy 0.8875\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0669\t Average training accuracy 0.8876\n",
      "Epoch [12]\t Average validation loss 0.0704\t Average validation accuracy 0.9190\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0713\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0685\t Accuracy 0.9004\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0684\t Accuracy 0.8951\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0680\t Accuracy 0.8927\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0679\t Accuracy 0.8946\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0680\t Accuracy 0.8949\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0679\t Accuracy 0.8941\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0677\t Accuracy 0.8926\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0677\t Accuracy 0.8926\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0677\t Accuracy 0.8917\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0676\t Accuracy 0.8908\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0677\t Average training accuracy 0.8910\n",
      "Epoch [13]\t Average validation loss 0.0712\t Average validation accuracy 0.9202\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0721\t Accuracy 0.9200\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0693\t Accuracy 0.9049\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0692\t Accuracy 0.9000\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0688\t Accuracy 0.8971\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0686\t Accuracy 0.8982\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0688\t Accuracy 0.8983\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0687\t Accuracy 0.8974\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0684\t Accuracy 0.8958\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0684\t Accuracy 0.8957\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0684\t Accuracy 0.8949\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0684\t Accuracy 0.8940\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0685\t Average training accuracy 0.8940\n",
      "Epoch [14]\t Average validation loss 0.0719\t Average validation accuracy 0.9216\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0730\t Accuracy 0.9300\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0701\t Accuracy 0.9084\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0699\t Accuracy 0.9029\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0695\t Accuracy 0.8997\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0693\t Accuracy 0.9007\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0695\t Accuracy 0.9006\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0694\t Accuracy 0.8997\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0691\t Accuracy 0.8981\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0691\t Accuracy 0.8980\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0691\t Accuracy 0.8976\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0690\t Accuracy 0.8967\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0691\t Average training accuracy 0.8966\n",
      "Epoch [15]\t Average validation loss 0.0726\t Average validation accuracy 0.9228\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0737\t Accuracy 0.9400\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0708\t Accuracy 0.9112\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0706\t Accuracy 0.9045\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0702\t Accuracy 0.9013\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0700\t Accuracy 0.9025\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0701\t Accuracy 0.9027\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0701\t Accuracy 0.9016\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0698\t Accuracy 0.9003\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0698\t Accuracy 0.9002\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0697\t Accuracy 0.8998\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0697\t Accuracy 0.8989\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0698\t Average training accuracy 0.8989\n",
      "Epoch [16]\t Average validation loss 0.0733\t Average validation accuracy 0.9240\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0744\t Accuracy 0.9400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0715\t Accuracy 0.9139\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0712\t Accuracy 0.9067\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0708\t Accuracy 0.9040\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0706\t Accuracy 0.9054\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0707\t Accuracy 0.9056\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0707\t Accuracy 0.9045\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0704\t Accuracy 0.9031\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0704\t Accuracy 0.9031\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0703\t Accuracy 0.9025\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0703\t Accuracy 0.9015\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0704\t Average training accuracy 0.9015\n",
      "Epoch [17]\t Average validation loss 0.0739\t Average validation accuracy 0.9270\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0751\t Accuracy 0.9400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0721\t Accuracy 0.9163\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0718\t Accuracy 0.9088\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0714\t Accuracy 0.9063\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0712\t Accuracy 0.9075\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0713\t Accuracy 0.9076\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0713\t Accuracy 0.9064\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0709\t Accuracy 0.9053\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0710\t Accuracy 0.9052\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0709\t Accuracy 0.9049\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0708\t Accuracy 0.9039\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0709\t Average training accuracy 0.9038\n",
      "Epoch [18]\t Average validation loss 0.0744\t Average validation accuracy 0.9290\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0757\t Accuracy 0.9400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0726\t Accuracy 0.9182\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0723\t Accuracy 0.9104\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0719\t Accuracy 0.9080\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0717\t Accuracy 0.9091\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0718\t Accuracy 0.9094\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0718\t Accuracy 0.9084\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0715\t Accuracy 0.9072\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0715\t Accuracy 0.9070\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0714\t Accuracy 0.9068\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0714\t Accuracy 0.9060\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0714\t Average training accuracy 0.9060\n",
      "Epoch [19]\t Average validation loss 0.0750\t Average validation accuracy 0.9316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9150.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework2-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Hidden-Layer MLP with Euclidean Loss ( ReLU and Sigmoid )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "twolayer = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "twolayer.add(FCLayer(784, 128))\n",
    "twolayer.add(SigmoidLayer())\n",
    "twolayer.add(ReLULayer())\n",
    "twolayer.add(FCLayer(128, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss -0.1460\t Accuracy 0.0600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss -0.0580\t Accuracy 0.1443\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss -0.0245\t Accuracy 0.1813\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss -0.0132\t Accuracy 0.2111\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss -0.0079\t Accuracy 0.2422\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss -0.0047\t Accuracy 0.2722\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss -0.0026\t Accuracy 0.3002\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss -0.0012\t Accuracy 0.3225\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss -0.0001\t Accuracy 0.3413\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0007\t Accuracy 0.3602\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0012\t Accuracy 0.3777\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0017\t Average training accuracy 0.3950\n",
      "Epoch [0]\t Average validation loss 0.0062\t Average validation accuracy 0.6092\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0071\t Accuracy 0.5700\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0060\t Accuracy 0.5871\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0062\t Accuracy 0.5937\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0063\t Accuracy 0.5895\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0061\t Accuracy 0.5937\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0060\t Accuracy 0.6006\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0059\t Accuracy 0.6076\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0058\t Accuracy 0.6101\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0057\t Accuracy 0.6137\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0056\t Accuracy 0.6177\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0055\t Accuracy 0.6217\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0054\t Average training accuracy 0.6278\n",
      "Epoch [1]\t Average validation loss 0.0038\t Average validation accuracy 0.7216\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0048\t Accuracy 0.7500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0041\t Accuracy 0.6931\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0043\t Accuracy 0.6918\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0044\t Accuracy 0.6856\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0042\t Accuracy 0.6880\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0042\t Accuracy 0.6923\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0041\t Accuracy 0.6957\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0040\t Accuracy 0.6950\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0040\t Accuracy 0.6971\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0039\t Accuracy 0.6989\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0038\t Accuracy 0.7003\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0037\t Average training accuracy 0.7040\n",
      "Epoch [2]\t Average validation loss 0.0023\t Average validation accuracy 0.7738\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0033\t Accuracy 0.7700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0028\t Accuracy 0.7433\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0030\t Accuracy 0.7424\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0031\t Accuracy 0.7358\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0030\t Accuracy 0.7365\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0029\t Accuracy 0.7384\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0029\t Accuracy 0.7402\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0028\t Accuracy 0.7384\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0028\t Accuracy 0.7401\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0027\t Accuracy 0.7412\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0027\t Accuracy 0.7420\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0026\t Average training accuracy 0.7446\n",
      "Epoch [3]\t Average validation loss 0.0014\t Average validation accuracy 0.8092\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0023\t Accuracy 0.8200\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0020\t Accuracy 0.7706\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0021\t Accuracy 0.7721\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0022\t Accuracy 0.7640\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0021\t Accuracy 0.7645\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0021\t Accuracy 0.7663\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0020\t Accuracy 0.7674\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0020\t Accuracy 0.7655\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0020\t Accuracy 0.7668\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0019\t Accuracy 0.7676\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0019\t Accuracy 0.7679\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0019\t Average training accuracy 0.7697\n",
      "Epoch [4]\t Average validation loss 0.0009\t Average validation accuracy 0.8282\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0016\t Accuracy 0.8400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0014\t Accuracy 0.7894\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0015\t Accuracy 0.7908\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0016\t Accuracy 0.7822\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0015\t Accuracy 0.7833\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0015\t Accuracy 0.7845\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0015\t Accuracy 0.7855\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0014\t Accuracy 0.7833\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0014\t Accuracy 0.7842\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0014\t Accuracy 0.7851\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0014\t Accuracy 0.7850\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0013\t Average training accuracy 0.7864\n",
      "Epoch [5]\t Average validation loss 0.0005\t Average validation accuracy 0.8426\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0011\t Accuracy 0.8600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0010\t Accuracy 0.8008\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0011\t Accuracy 0.8023\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0012\t Accuracy 0.7942\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0011\t Accuracy 0.7944\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0011\t Accuracy 0.7953\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0011\t Accuracy 0.7960\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0011\t Accuracy 0.7942\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0011\t Accuracy 0.7951\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0010\t Accuracy 0.7958\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0010\t Accuracy 0.7960\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0010\t Average training accuracy 0.7971\n",
      "Epoch [6]\t Average validation loss 0.0003\t Average validation accuracy 0.8536\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0008\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0008\t Accuracy 0.8106\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0008\t Accuracy 0.8127\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0009\t Accuracy 0.8041\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0008\t Accuracy 0.8042\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0008\t Accuracy 0.8045\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0008\t Accuracy 0.8048\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0008\t Accuracy 0.8029\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0008\t Accuracy 0.8039\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0008\t Accuracy 0.8044\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0008\t Accuracy 0.8041\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0007\t Average training accuracy 0.8049\n",
      "Epoch [7]\t Average validation loss 0.0002\t Average validation accuracy 0.8582\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0006\t Accuracy 0.8600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0006\t Accuracy 0.8192\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0006\t Accuracy 0.8198\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0007\t Accuracy 0.8116\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0006\t Accuracy 0.8112\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0006\t Accuracy 0.8109\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0006\t Accuracy 0.8109\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0006\t Accuracy 0.8088\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0006\t Accuracy 0.8097\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0006\t Accuracy 0.8101\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0006\t Accuracy 0.8098\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0006\t Average training accuracy 0.8105\n",
      "Epoch [8]\t Average validation loss 0.0002\t Average validation accuracy 0.8624\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0004\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0004\t Accuracy 0.8210\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0005\t Accuracy 0.8229\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0005\t Accuracy 0.8162\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0005\t Accuracy 0.8158\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0005\t Accuracy 0.8153\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0005\t Accuracy 0.8153\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0005\t Accuracy 0.8131\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0005\t Accuracy 0.8142\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0005\t Accuracy 0.8146\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0004\t Accuracy 0.8141\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0004\t Average training accuracy 0.8147\n",
      "Epoch [9]\t Average validation loss 0.0002\t Average validation accuracy 0.8660\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0003\t Accuracy 0.8700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0003\t Accuracy 0.8237\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0004\t Accuracy 0.8252\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0004\t Accuracy 0.8193\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0004\t Accuracy 0.8195\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0004\t Accuracy 0.8189\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0004\t Accuracy 0.8188\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0004\t Accuracy 0.8164\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0004\t Accuracy 0.8175\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0004\t Accuracy 0.8180\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0004\t Accuracy 0.8176\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0003\t Average training accuracy 0.8183\n",
      "Epoch [10]\t Average validation loss 0.0001\t Average validation accuracy 0.8708\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0003\t Accuracy 0.8700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0003\t Accuracy 0.8261\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0003\t Accuracy 0.8273\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0003\t Accuracy 0.8217\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0003\t Accuracy 0.8217\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0003\t Accuracy 0.8210\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0003\t Accuracy 0.8208\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0003\t Accuracy 0.8186\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0003\t Accuracy 0.8195\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0003\t Accuracy 0.8200\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0003\t Accuracy 0.8197\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0003\t Average training accuracy 0.8203\n",
      "Epoch [11]\t Average validation loss 0.0001\t Average validation accuracy 0.8730\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0003\t Accuracy 0.8700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0002\t Accuracy 0.8280\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0002\t Accuracy 0.8295\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0003\t Accuracy 0.8240\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0003\t Accuracy 0.8242\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0002\t Accuracy 0.8234\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0002\t Accuracy 0.8228\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0002\t Accuracy 0.8206\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0002\t Accuracy 0.8215\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0002\t Accuracy 0.8220\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0002\t Accuracy 0.8217\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0002\t Average training accuracy 0.8222\n",
      "Epoch [12]\t Average validation loss 0.0001\t Average validation accuracy 0.8762\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.8900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0002\t Accuracy 0.8302\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0002\t Accuracy 0.8318\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0002\t Accuracy 0.8266\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0002\t Accuracy 0.8265\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0002\t Accuracy 0.8257\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0002\t Accuracy 0.8247\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0002\t Accuracy 0.8226\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0002\t Accuracy 0.8232\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0002\t Accuracy 0.8236\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0002\t Accuracy 0.8233\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0002\t Average training accuracy 0.8237\n",
      "Epoch [13]\t Average validation loss 0.0002\t Average validation accuracy 0.8776\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.8900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0002\t Accuracy 0.8318\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0002\t Accuracy 0.8333\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0002\t Accuracy 0.8280\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0002\t Accuracy 0.8277\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0002\t Accuracy 0.8268\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0002\t Accuracy 0.8257\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0002\t Accuracy 0.8238\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0002\t Accuracy 0.8244\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0002\t Accuracy 0.8248\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0002\t Accuracy 0.8247\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0002\t Average training accuracy 0.8250\n",
      "Epoch [14]\t Average validation loss 0.0002\t Average validation accuracy 0.8782\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.8322\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.8342\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0002\t Accuracy 0.8288\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.8286\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.8277\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.8265\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.8245\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.8250\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.8255\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.8254\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0001\t Average training accuracy 0.8257\n",
      "Epoch [15]\t Average validation loss 0.0002\t Average validation accuracy 0.8790\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.8322\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.8341\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.8288\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.8288\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.8282\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.8269\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.8247\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.8252\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.8256\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.8255\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0001\t Average training accuracy 0.8258\n",
      "Epoch [16]\t Average validation loss 0.0002\t Average validation accuracy 0.8812\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.8324\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.8337\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.8286\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.8284\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.8276\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.8266\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.8246\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.8250\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.8253\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.8252\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0001\t Average training accuracy 0.8255\n",
      "Epoch [17]\t Average validation loss 0.0002\t Average validation accuracy 0.8830\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.8800\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.8308\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.8335\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.8283\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.8281\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.8273\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.8264\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.8246\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.8249\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.8251\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.8250\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0001\t Average training accuracy 0.8253\n",
      "Epoch [18]\t Average validation loss 0.0002\t Average validation accuracy 0.8840\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0002\t Accuracy 0.8800\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0001\t Accuracy 0.8312\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0001\t Accuracy 0.8323\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0001\t Accuracy 0.8274\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0001\t Accuracy 0.8272\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0001\t Accuracy 0.8261\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0001\t Accuracy 0.8254\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0001\t Accuracy 0.8233\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0001\t Accuracy 0.8238\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0001\t Accuracy 0.8239\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0001\t Accuracy 0.8235\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0001\t Average training accuracy 0.8239\n",
      "Epoch [19]\t Average validation loss 0.0001\t Average validation accuracy 0.8834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twolayer, twolayer_loss, twolayer_acc = train(twolayer, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8547.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(twolayer, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two hidden layer MLP with Softmax Cross-Entropy Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "twolayers = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "twolayers.add(FCLayer(784, 128))\n",
    "twolayers.add(SigmoidLayer())\n",
    "twolayers.add(ReLULayer())\n",
    "twolayers.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.0096\t Accuracy 0.0800\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0118\t Accuracy 0.1322\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0115\t Accuracy 0.1712\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0118\t Accuracy 0.1930\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0125\t Accuracy 0.2170\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0133\t Accuracy 0.2408\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0141\t Accuracy 0.2634\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0148\t Accuracy 0.2814\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0155\t Accuracy 0.2981\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0162\t Accuracy 0.3142\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0169\t Accuracy 0.3315\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0176\t Average training accuracy 0.3480\n",
      "Epoch [0]\t Average validation loss 0.0255\t Average validation accuracy 0.5572\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0245\t Accuracy 0.4900\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0251\t Accuracy 0.5355\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0253\t Accuracy 0.5395\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0255\t Accuracy 0.5396\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0261\t Accuracy 0.5485\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0265\t Accuracy 0.5556\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0271\t Accuracy 0.5637\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0275\t Accuracy 0.5687\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0279\t Accuracy 0.5745\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0282\t Accuracy 0.5793\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0287\t Accuracy 0.5848\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0291\t Average training accuracy 0.5920\n",
      "Epoch [1]\t Average validation loss 0.0345\t Average validation accuracy 0.7006\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0327\t Accuracy 0.6800\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0333\t Accuracy 0.6657\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0333\t Accuracy 0.6688\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0331\t Accuracy 0.6618\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0336\t Accuracy 0.6648\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0339\t Accuracy 0.6688\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0343\t Accuracy 0.6722\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0345\t Accuracy 0.6734\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0347\t Accuracy 0.6777\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0349\t Accuracy 0.6807\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0352\t Accuracy 0.6828\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0354\t Average training accuracy 0.6875\n",
      "Epoch [2]\t Average validation loss 0.0398\t Average validation accuracy 0.7670\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0377\t Accuracy 0.7700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0382\t Accuracy 0.7337\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0380\t Accuracy 0.7356\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0377\t Accuracy 0.7277\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0381\t Accuracy 0.7294\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0382\t Accuracy 0.7302\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0386\t Accuracy 0.7318\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0387\t Accuracy 0.7332\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0388\t Accuracy 0.7357\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0389\t Accuracy 0.7376\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0391\t Accuracy 0.7386\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0393\t Average training accuracy 0.7419\n",
      "Epoch [3]\t Average validation loss 0.0431\t Average validation accuracy 0.8012\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0409\t Accuracy 0.8000\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0413\t Accuracy 0.7690\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0411\t Accuracy 0.7728\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0407\t Accuracy 0.7654\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0410\t Accuracy 0.7656\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0411\t Accuracy 0.7666\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0414\t Accuracy 0.7667\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0415\t Accuracy 0.7670\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0415\t Accuracy 0.7686\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0416\t Accuracy 0.7690\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0417\t Accuracy 0.7695\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0419\t Average training accuracy 0.7716\n",
      "Epoch [4]\t Average validation loss 0.0454\t Average validation accuracy 0.8256\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0433\t Accuracy 0.8400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0435\t Accuracy 0.7886\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0432\t Accuracy 0.7946\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0428\t Accuracy 0.7879\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0431\t Accuracy 0.7876\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0432\t Accuracy 0.7877\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0434\t Accuracy 0.7882\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0434\t Accuracy 0.7876\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0434\t Accuracy 0.7887\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0435\t Accuracy 0.7887\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0436\t Accuracy 0.7891\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0438\t Average training accuracy 0.7906\n",
      "Epoch [5]\t Average validation loss 0.0471\t Average validation accuracy 0.8416\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0451\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0452\t Accuracy 0.8016\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0449\t Accuracy 0.8072\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0444\t Accuracy 0.8011\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0446\t Accuracy 0.8007\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0447\t Accuracy 0.8005\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0449\t Accuracy 0.8010\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0449\t Accuracy 0.7997\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0449\t Accuracy 0.8005\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0449\t Accuracy 0.8000\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0450\t Accuracy 0.8002\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0452\t Average training accuracy 0.8015\n",
      "Epoch [6]\t Average validation loss 0.0484\t Average validation accuracy 0.8496\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0464\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0464\t Accuracy 0.8114\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0461\t Accuracy 0.8160\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0457\t Accuracy 0.8087\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0459\t Accuracy 0.8078\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0459\t Accuracy 0.8081\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0461\t Accuracy 0.8085\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0461\t Accuracy 0.8075\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0460\t Accuracy 0.8083\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0461\t Accuracy 0.8079\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0461\t Accuracy 0.8079\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0463\t Average training accuracy 0.8091\n",
      "Epoch [7]\t Average validation loss 0.0494\t Average validation accuracy 0.8558\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0475\t Accuracy 0.8600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0474\t Accuracy 0.8196\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0471\t Accuracy 0.8222\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0467\t Accuracy 0.8147\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0468\t Accuracy 0.8137\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0469\t Accuracy 0.8140\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0470\t Accuracy 0.8146\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0470\t Accuracy 0.8135\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0469\t Accuracy 0.8141\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0469\t Accuracy 0.8135\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0470\t Accuracy 0.8134\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0471\t Average training accuracy 0.8142\n",
      "Epoch [8]\t Average validation loss 0.0501\t Average validation accuracy 0.8606\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0484\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0482\t Accuracy 0.8237\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0479\t Accuracy 0.8261\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0475\t Accuracy 0.8186\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0476\t Accuracy 0.8180\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0476\t Accuracy 0.8181\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0478\t Accuracy 0.8186\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0477\t Accuracy 0.8180\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0477\t Accuracy 0.8186\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0477\t Accuracy 0.8178\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0477\t Accuracy 0.8178\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0478\t Average training accuracy 0.8184\n",
      "Epoch [9]\t Average validation loss 0.0508\t Average validation accuracy 0.8648\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0491\t Accuracy 0.8800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0489\t Accuracy 0.8282\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0486\t Accuracy 0.8294\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0481\t Accuracy 0.8219\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0482\t Accuracy 0.8214\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0483\t Accuracy 0.8218\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0484\t Accuracy 0.8221\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0483\t Accuracy 0.8212\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0483\t Accuracy 0.8217\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0482\t Accuracy 0.8208\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0483\t Accuracy 0.8208\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0484\t Average training accuracy 0.8214\n",
      "Epoch [10]\t Average validation loss 0.0512\t Average validation accuracy 0.8682\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0497\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0494\t Accuracy 0.8322\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0491\t Accuracy 0.8327\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0487\t Accuracy 0.8252\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0487\t Accuracy 0.8246\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0488\t Accuracy 0.8245\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0489\t Accuracy 0.8249\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0488\t Accuracy 0.8240\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0487\t Accuracy 0.8245\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0487\t Accuracy 0.8236\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0487\t Accuracy 0.8235\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0488\t Average training accuracy 0.8242\n",
      "Epoch [11]\t Average validation loss 0.0516\t Average validation accuracy 0.8700\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0502\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0498\t Accuracy 0.8351\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0496\t Accuracy 0.8348\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0491\t Accuracy 0.8273\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0491\t Accuracy 0.8267\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0492\t Accuracy 0.8265\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0493\t Accuracy 0.8266\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0491\t Accuracy 0.8258\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0491\t Accuracy 0.8264\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0491\t Accuracy 0.8256\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0491\t Accuracy 0.8254\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0492\t Average training accuracy 0.8262\n",
      "Epoch [12]\t Average validation loss 0.0519\t Average validation accuracy 0.8736\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0506\t Accuracy 0.8900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0501\t Accuracy 0.8369\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0499\t Accuracy 0.8354\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0495\t Accuracy 0.8281\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0495\t Accuracy 0.8274\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0495\t Accuracy 0.8273\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0496\t Accuracy 0.8275\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0494\t Accuracy 0.8270\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0494\t Accuracy 0.8276\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0494\t Accuracy 0.8269\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0494\t Accuracy 0.8266\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0495\t Average training accuracy 0.8274\n",
      "Epoch [13]\t Average validation loss 0.0522\t Average validation accuracy 0.8738\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0509\t Accuracy 0.8900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0504\t Accuracy 0.8375\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0502\t Accuracy 0.8368\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0498\t Accuracy 0.8294\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0497\t Accuracy 0.8287\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0498\t Accuracy 0.8284\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0499\t Accuracy 0.8287\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0497\t Accuracy 0.8280\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0497\t Accuracy 0.8286\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0496\t Accuracy 0.8278\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0496\t Accuracy 0.8276\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0497\t Average training accuracy 0.8284\n",
      "Epoch [14]\t Average validation loss 0.0523\t Average validation accuracy 0.8750\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0511\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0506\t Accuracy 0.8376\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0504\t Accuracy 0.8377\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0500\t Accuracy 0.8299\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0500\t Accuracy 0.8293\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0500\t Accuracy 0.8290\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0501\t Accuracy 0.8290\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0499\t Accuracy 0.8285\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0499\t Accuracy 0.8291\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0498\t Accuracy 0.8284\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0498\t Accuracy 0.8282\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0499\t Average training accuracy 0.8290\n",
      "Epoch [15]\t Average validation loss 0.0525\t Average validation accuracy 0.8764\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0513\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0508\t Accuracy 0.8369\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0506\t Accuracy 0.8371\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0502\t Accuracy 0.8301\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0501\t Accuracy 0.8296\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0502\t Accuracy 0.8292\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0502\t Accuracy 0.8293\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0500\t Accuracy 0.8289\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0500\t Accuracy 0.8293\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0500\t Accuracy 0.8286\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0500\t Accuracy 0.8284\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0500\t Average training accuracy 0.8291\n",
      "Epoch [16]\t Average validation loss 0.0526\t Average validation accuracy 0.8780\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0514\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0509\t Accuracy 0.8378\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0507\t Accuracy 0.8380\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0503\t Accuracy 0.8310\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0503\t Accuracy 0.8303\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0503\t Accuracy 0.8298\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0504\t Accuracy 0.8300\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0502\t Accuracy 0.8293\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0502\t Accuracy 0.8298\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0501\t Accuracy 0.8290\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0501\t Accuracy 0.8289\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0501\t Average training accuracy 0.8295\n",
      "Epoch [17]\t Average validation loss 0.0526\t Average validation accuracy 0.8804\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0515\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0510\t Accuracy 0.8382\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0509\t Accuracy 0.8386\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0505\t Accuracy 0.8319\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0504\t Accuracy 0.8313\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0504\t Accuracy 0.8305\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0505\t Accuracy 0.8304\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0503\t Accuracy 0.8297\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0502\t Accuracy 0.8302\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0502\t Accuracy 0.8294\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0502\t Accuracy 0.8293\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0502\t Average training accuracy 0.8297\n",
      "Epoch [18]\t Average validation loss 0.0527\t Average validation accuracy 0.8822\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0516\t Accuracy 0.8900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0510\t Accuracy 0.8375\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0509\t Accuracy 0.8379\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0505\t Accuracy 0.8315\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0504\t Accuracy 0.8309\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0505\t Accuracy 0.8304\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0505\t Accuracy 0.8301\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0503\t Accuracy 0.8293\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0503\t Accuracy 0.8299\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0502\t Accuracy 0.8290\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0502\t Accuracy 0.8289\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0503\t Average training accuracy 0.8293\n",
      "Epoch [19]\t Average validation loss 0.0527\t Average validation accuracy 0.8832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twolayers, twolayers_loss, twolayers_acc = train(twolayers, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8604.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(twolayers, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Euclidean': [twolayer_loss, twolayer_acc],\n",
    "                   'Softmax CSE': [twolayers_loss, twolayers_acc]})"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
